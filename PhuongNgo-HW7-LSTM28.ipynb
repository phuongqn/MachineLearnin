{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=['AIIMAT.txt', 'MLOE.txt','OKEWFSMP.txt','TAM.txt','TAMatter.txt', 'THWP.txt','TPP.txt']\n",
    "corpus=[]\n",
    "def get_corpus(path):\n",
    "    with open(path,'r', errors= 'ignore') as data:\n",
    "        corpus=data.read()\n",
    "        corpus=corpus.lower()\n",
    "    return corpus\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    path='/Users/phuongqn/Desktop/INF552/Homework/Homework 7 Data/Book Files/books/'+file_names[i]+''\n",
    "    corpus.append(get_corpus(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=:;<]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"<\", \" \", text)\n",
    "    text = re.sub(r\";\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"[0123456789]\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = clean_text(corpus[0])\n",
    "c1 = clean_text(corpus[1])\n",
    "c2 = clean_text(corpus[2])\n",
    "c3 = clean_text(corpus[3])\n",
    "c4 = clean_text(corpus[4])\n",
    "c5 = clean_text(corpus[5])\n",
    "c6 = clean_text(corpus[6])\n",
    "corpus_c=[c0,c1,c2,c3,c4,c5,c6]\n",
    "t = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=False, split=' ', char_level=True, oov_token=None, document_count=0)\n",
    "t.fit_on_texts(corpus_c)\n",
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=[]\n",
    "for i in corpus_c:\n",
    "    chars.extend(list(set(set(i))))\n",
    "    \n",
    "chars = sorted(list(set(chars)))\n",
    "char_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_o(file, size):\n",
    "    ip=[]\n",
    "    op=[]\n",
    "    for i in range(0, len(file) - size+1, 1):\n",
    "        seq_in = file[i:i + size - 1]\n",
    "        seq_out = file[i + size-1]\n",
    "        ip.append([char_int[c] for c in seq_in])\n",
    "        op.append(char_int[seq_out])\n",
    "    return ip, op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_seq=[]\n",
    "op_char=[]\n",
    "for c in corpus_c:\n",
    "    w=100\n",
    "    ip, op = i_o(c, w)\n",
    "    ip_seq.extend(ip)\n",
    "    op_char.extend(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4850971, 99, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X\n",
    "X = np.reshape(ip_seq, (len(ip_seq), 99,1))\n",
    "# normalize\n",
    "X = X / float(len(chars))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(op_char)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I noticed that using an LSTM with N=256 will typically yield better results, but the run time is long (4hrs per epoch). I have attached that version, which I stopped prematurely, for comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 28)                3360      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                783       \n",
      "=================================================================\n",
      "Total params: 4,143\n",
      "Trainable params: 4,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LSTMmodel = Sequential()\n",
    "LSTMmodel.add(LSTM(28, input_shape=(X.shape[1], X.shape[2])))\n",
    "# LSTMmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), dropout=0.2))\n",
    "LSTMmodel.add(Dropout(0.2))\n",
    "LSTMmodel.add(Dense(y.shape[1], activation='softmax'))\n",
    "print(LSTMmodel.summary())\n",
    "LSTMmodel.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath= '/Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4850971 samples\n",
      "Epoch 1/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.5233\n",
      "Epoch 00001: loss improved from inf to 2.52329, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-01-2.5233.hdf5\n",
      "4850971/4850971 [==============================] - 2431s 501us/sample - loss: 2.5233\n",
      "Epoch 2/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.3643\n",
      "Epoch 00002: loss improved from 2.52329 to 2.36433, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-02-2.3643.hdf5\n",
      "4850971/4850971 [==============================] - 2408s 496us/sample - loss: 2.3643\n",
      "Epoch 3/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.3006\n",
      "Epoch 00003: loss improved from 2.36433 to 2.30056, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-03-2.3006.hdf5\n",
      "4850971/4850971 [==============================] - 2391s 493us/sample - loss: 2.3006\n",
      "Epoch 4/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.2681\n",
      "Epoch 00004: loss improved from 2.30056 to 2.26814, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-04-2.2681.hdf5\n",
      "4850971/4850971 [==============================] - 4889s 1ms/sample - loss: 2.2681\n",
      "Epoch 5/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.2462\n",
      "Epoch 00005: loss improved from 2.26814 to 2.24621, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-05-2.2462.hdf5\n",
      "4850971/4850971 [==============================] - 2358s 486us/sample - loss: 2.2462\n",
      "Epoch 6/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.2309\n",
      "Epoch 00006: loss improved from 2.24621 to 2.23089, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-06-2.2309.hdf5\n",
      "4850971/4850971 [==============================] - 2290s 472us/sample - loss: 2.2309\n",
      "Epoch 7/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.2187\n",
      "Epoch 00007: loss improved from 2.23089 to 2.21870, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-07-2.2187.hdf5\n",
      "4850971/4850971 [==============================] - 2290s 472us/sample - loss: 2.2187\n",
      "Epoch 8/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.2095\n",
      "Epoch 00008: loss improved from 2.21870 to 2.20947, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-08-2.2095.hdf5\n",
      "4850971/4850971 [==============================] - 2293s 473us/sample - loss: 2.2095\n",
      "Epoch 9/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.2007\n",
      "Epoch 00009: loss improved from 2.20947 to 2.20074, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-09-2.2007.hdf5\n",
      "4850971/4850971 [==============================] - 2294s 473us/sample - loss: 2.2007\n",
      "Epoch 10/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1939\n",
      "Epoch 00010: loss improved from 2.20074 to 2.19387, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-10-2.1939.hdf5\n",
      "4850971/4850971 [==============================] - 2307s 476us/sample - loss: 2.1939\n",
      "Epoch 11/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1878\n",
      "Epoch 00011: loss improved from 2.19387 to 2.18783, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-11-2.1878.hdf5\n",
      "4850971/4850971 [==============================] - 2305s 475us/sample - loss: 2.1878\n",
      "Epoch 12/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1824\n",
      "Epoch 00012: loss improved from 2.18783 to 2.18245, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-12-2.1824.hdf5\n",
      "4850971/4850971 [==============================] - 2336s 481us/sample - loss: 2.1824\n",
      "Epoch 13/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1777\n",
      "Epoch 00013: loss improved from 2.18245 to 2.17771, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-13-2.1777.hdf5\n",
      "4850971/4850971 [==============================] - 2320s 478us/sample - loss: 2.1777\n",
      "Epoch 14/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1734\n",
      "Epoch 00014: loss improved from 2.17771 to 2.17341, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-14-2.1734.hdf5\n",
      "4850971/4850971 [==============================] - 2309s 476us/sample - loss: 2.1734\n",
      "Epoch 15/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1694\n",
      "Epoch 00015: loss improved from 2.17341 to 2.16944, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-15-2.1694.hdf5\n",
      "4850971/4850971 [==============================] - 2322s 479us/sample - loss: 2.1694\n",
      "Epoch 16/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1660\n",
      "Epoch 00016: loss improved from 2.16944 to 2.16596, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-16-2.1660.hdf5\n",
      "4850971/4850971 [==============================] - 2318s 478us/sample - loss: 2.1660\n",
      "Epoch 17/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1640\n",
      "Epoch 00017: loss improved from 2.16596 to 2.16402, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-17-2.1640.hdf5\n",
      "4850971/4850971 [==============================] - 2346s 484us/sample - loss: 2.1640\n",
      "Epoch 18/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1613\n",
      "Epoch 00018: loss improved from 2.16402 to 2.16126, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-18-2.1613.hdf5\n",
      "4850971/4850971 [==============================] - 2291s 472us/sample - loss: 2.1613\n",
      "Epoch 19/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1587\n",
      "Epoch 00019: loss improved from 2.16126 to 2.15869, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-19-2.1587.hdf5\n",
      "4850971/4850971 [==============================] - 2251s 464us/sample - loss: 2.1587\n",
      "Epoch 20/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1568\n",
      "Epoch 00020: loss improved from 2.15869 to 2.15677, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-20-2.1568.hdf5\n",
      "4850971/4850971 [==============================] - 2252s 464us/sample - loss: 2.1568\n",
      "Epoch 21/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1553\n",
      "Epoch 00021: loss improved from 2.15677 to 2.15529, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-21-2.1553.hdf5\n",
      "4850971/4850971 [==============================] - 2250s 464us/sample - loss: 2.1553\n",
      "Epoch 22/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1538\n",
      "Epoch 00022: loss improved from 2.15529 to 2.15375, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-22-2.1538.hdf5\n",
      "4850971/4850971 [==============================] - 2253s 464us/sample - loss: 2.1538\n",
      "Epoch 23/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1524\n",
      "Epoch 00023: loss improved from 2.15375 to 2.15240, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-23-2.1524.hdf5\n",
      "4850971/4850971 [==============================] - 2251s 464us/sample - loss: 2.1524\n",
      "Epoch 24/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1499\n",
      "Epoch 00024: loss improved from 2.15240 to 2.14990, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-24-2.1499.hdf5\n",
      "4850971/4850971 [==============================] - 8493s 2ms/sample - loss: 2.1499\n",
      "Epoch 25/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1497\n",
      "Epoch 00025: loss improved from 2.14990 to 2.14967, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-25-2.1497.hdf5\n",
      "4850971/4850971 [==============================] - 2394s 494us/sample - loss: 2.1497\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1477\n",
      "Epoch 00026: loss improved from 2.14967 to 2.14774, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-26-2.1477.hdf5\n",
      "4850971/4850971 [==============================] - 2361s 487us/sample - loss: 2.1477\n",
      "Epoch 27/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1468\n",
      "Epoch 00027: loss improved from 2.14774 to 2.14681, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-27-2.1468.hdf5\n",
      "4850971/4850971 [==============================] - 2699s 556us/sample - loss: 2.1468\n",
      "Epoch 28/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1449\n",
      "Epoch 00028: loss improved from 2.14681 to 2.14491, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-28-2.1449.hdf5\n",
      "4850971/4850971 [==============================] - 3178s 655us/sample - loss: 2.1449\n",
      "Epoch 29/30\n",
      "4850880/4850971 [============================>.] - ETA: 0s - loss: 2.1440\n",
      "Epoch 00029: loss improved from 2.14491 to 2.14401, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-29-2.1440.hdf5\n",
      "4850971/4850971 [==============================] - 3957s 816us/sample - loss: 2.1440\n",
      "Epoch 30/30\n",
      "4850944/4850971 [============================>.] - ETA: 0s - loss: 2.1433\n",
      "Epoch 00030: loss improved from 2.14401 to 2.14327, saving model to /Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-30-2.1433.hdf5\n",
      "4850971/4850971 [==============================] - 2385s 492us/sample - loss: 2.1433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63f8e2c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTMmodel.fit(X, y, epochs=30, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight='/Users/phuongqn/Desktop/INF552/HW-Phuong/LSTMweights/weights-improvement-30-2.1433.hdf5'\n",
    "LSTMmodel.load_weights(weight)\n",
    "LSTMmodel.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena This school of psychologists tends not to emphasize the object the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the so the s\n"
     ]
    }
   ],
   "source": [
    "seed = 'There are those who take mental phenomena naively, just as they would physical phenomena This school of psychologists tends not to emphasize the object'\n",
    "\n",
    "pattern = [char_int[c] for c in seed[-99:].lower()]\n",
    "\n",
    "for i in range(1000):\n",
    "    seq = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    seq = seq / float(len(chars))\n",
    "    charpredict = LSTMmodel.predict(seq, verbose=0)\n",
    "    ind = np.argmax(charpredict)\n",
    "    seed+=int_char[ind]\n",
    "    pattern.append(ind)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
